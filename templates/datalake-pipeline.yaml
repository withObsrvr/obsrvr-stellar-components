apiVersion: flowctl/v1
kind: Pipeline
metadata:
  name: stellar-datalake-pipeline
  description: "Process TTP events from Stellar data lake storage for historical analysis"
  version: v1.0.0
  labels:
    component-bundle: obsrvr/obsrvr-stellar-suite
    template: datalake-pipeline
    mode: batch
    
spec:
  # Execution configuration
  execution:
    mode: ${EXECUTION_MODE:-native}
    
  # Pipeline configuration
  config:
    # Stellar network configuration
    stellar:
      network: ${STELLAR_NETWORK:-mainnet}
      network_passphrase: ${NETWORK_PASSPHRASE:-Public Global Stellar Network ; September 2015}
      
    # Storage configuration
    storage:
      backend: ${STORAGE_BACKEND:-s3}
      bucket_name: ${BUCKET_NAME}
      aws_region: ${AWS_REGION:-us-west-2}
      gcp_project: ${GCP_PROJECT}
      storage_path: ${STORAGE_PATH:-./data/ledgers}
      
    # Processing configuration
    processing:
      start_ledger: ${START_LEDGER:-1000000}
      end_ledger: ${END_LEDGER:-0}
      batch_size: ${BATCH_SIZE:-2000}
      processor_threads: ${PROCESSOR_THREADS:-8}
      concurrent_readers: ${CONCURRENT_READERS:-4}
      event_types: ${EVENT_TYPES:-payment,path_payment_strict_receive,path_payment_strict_send}
      
    # Output configuration
    output:
      formats: ${OUTPUT_FORMATS:-parquet}
      data_dir: ${DATA_DIR:-./data}
      compression: ${COMPRESSION:-snappy}
      partition_strategy: ${PARTITION_BY:-date}
      
    # Performance tuning
    performance:
      buffer_size: ${BUFFER_SIZE:-20000}
      flush_interval: ${FLUSH_INTERVAL:-60s}
      
  # Component definitions
  sources:
    - name: stellar-datalake-source
      component: obsrvr/stellar-arrow-source@v1.0.0
      config:
        # Source configuration
        source_type: datalake
        storage_backend: ${STORAGE_BACKEND:-s3}
        bucket_name: ${BUCKET_NAME}
        network_passphrase: ${NETWORK_PASSPHRASE:-Public Global Stellar Network ; September 2015}
        
        # AWS S3 configuration
        aws_region: ${AWS_REGION:-us-west-2}
        
        # GCP configuration
        gcp_project: ${GCP_PROJECT}
        
        # Filesystem configuration
        storage_path: ${STORAGE_PATH:-./data/ledgers}
        
        # Processing range
        start_ledger: ${START_LEDGER:-1000000}
        end_ledger: ${END_LEDGER:-0}
        
        # Performance configuration
        batch_size: ${BATCH_SIZE:-2000}
        buffer_size: ${BUFFER_SIZE:-20000}
        concurrent_readers: ${CONCURRENT_READERS:-4}
        
        # Arrow configuration
        arrow_memory_pool: default
        compression: lz4
        
        # Monitoring
        health_port: 8088
        metrics_enabled: true
        log_level: ${LOG_LEVEL:-info}
        
      # Resource allocation for high-throughput batch processing
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
          storage: 10Gi
        limits:
          cpu: 2000m
          memory: 4Gi
          storage: 100Gi
          
      # Health checks
      health:
        readiness:
          path: /health
          port: 8088
          initial_delay: 30s
          
  processors:
    - name: ttp-batch-processor
      component: obsrvr/ttp-arrow-processor@v1.0.0
      inputs: [stellar-datalake-source]
      config:
        # Source connection
        source_endpoint: localhost:8815
        
        # Processing configuration optimized for batch
        event_types: 
          - payment
          - path_payment_strict_receive
          - path_payment_strict_send
        processor_threads: ${PROCESSOR_THREADS:-8}
        batch_size: ${BATCH_SIZE:-2000}
        buffer_size: ${BUFFER_SIZE:-20000}
        
        # Batch processing optimizations
        include_raw_xdr: false
        include_transaction_details: true
        deduplicate_events: true
        
        # Arrow compute configuration for high throughput
        compute_threads: 0 # Auto-detect
        memory_pool: jemalloc
        
        # Monitoring
        health_port: 8088
        metrics_enabled: true
        stats_interval: 60s
        log_level: ${LOG_LEVEL:-info}
        
      # Resource allocation for intensive processing
      resources:
        requests:
          cpu: 1000m
          memory: 2Gi
        limits:
          cpu: 4000m
          memory: 8Gi
          
      # Health checks
      health:
        readiness:
          path: /health
          port: 8088
          initial_delay: 30s
          
  sinks:
    - name: batch-analytics-sink
      component: obsrvr/arrow-analytics-sink@v1.0.0
      inputs: [ttp-batch-processor]
      config:
        # Source connection
        processor_endpoint: localhost:8816
        
        # Output formats optimized for batch analytics
        output_formats:
          - parquet
          - csv
          
        # Parquet configuration for analytics
        parquet_path: ${DATA_DIR:-./data}/ttp_events
        parquet_compression: ${COMPRESSION:-snappy}
        partition_by: [date, asset_code]
        parquet_batch_size: 50000
        
        # CSV configuration for external tools
        csv_path: ${DATA_DIR:-./data}/csv
        csv_delimiter: ","
        
        # Performance configuration for batch writes
        buffer_size: ${BUFFER_SIZE:-20000}
        writer_threads: 8
        flush_interval: ${FLUSH_INTERVAL:-60s}
        
        # Disable real-time features for batch processing
        real_time_analytics: false
        
        # Data retention for historical analysis
        retention_policy:
          enabled: ${ENABLE_RETENTION:-false}
          max_age: ${MAX_DATA_AGE:-365d}
          max_size: ${MAX_DATA_SIZE:-1TB}
        
        # Monitoring
        health_port: 8088
        metrics_enabled: true
        log_level: ${LOG_LEVEL:-info}
        
      # Resource allocation for high-volume writes
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
          storage: 100Gi
        limits:
          cpu: 2000m
          memory: 4Gi
          storage: 10Ti
          
      # Health checks
      health:
        readiness:
          path: /health
          port: 8088
          initial_delay: 15s
          
  # Pipeline-level monitoring
  monitoring:
    metrics:
      enabled: true
      port: 9090
      path: /metrics
      
    logging:
      level: ${LOG_LEVEL:-info}
      format: json
      
    # Enhanced batch processing metrics
    batch_metrics:
      enabled: true
      report_interval: 5m
      include_throughput: true
      include_progress: true
      
  # Networking configuration
  networking:
    ports:
      stellar-datalake-source: 8815
      ttp-batch-processor: 8816
      batch-analytics-sink: 8817
      health: 8088
      metrics: 9090
      
    endpoints:
      - name: health
        port: 8088
        path: /health
        public: false
        
      - name: metrics
        port: 9090
        path: /metrics
        public: false
        
  # Batch job configuration
  batch:
    enabled: true
    completion_policy: all_success
    parallelism: 1
    backoff_limit: 3
    active_deadline_seconds: 86400 # 24 hours
    
    # Job scheduling
    schedule:
      enabled: ${ENABLE_SCHEDULING:-false}
      cron: ${BATCH_SCHEDULE:-0 2 * * *} # Daily at 2 AM
      concurrency_policy: forbid
      
  # Environment-specific overrides
  environments:
    development:
      config:
        storage:
          backend: filesystem
          storage_path: ./test-data/ledgers
        processing:
          start_ledger: 1000000
          end_ledger: 1001000
          batch_size: 500
          processor_threads: 2
          concurrent_readers: 1
        output:
          formats: [json]
      resources:
        requests:
          cpu: 200m
          memory: 256Mi
        limits:
          cpu: 1000m
          memory: 1Gi
          
    staging:
      config:
        storage:
          backend: s3
          bucket_name: stellar-staging-ledgers
        processing:
          start_ledger: 45000000
          end_ledger: 45010000
          batch_size: 1000
          processor_threads: 4
          concurrent_readers: 2
        output:
          formats: [parquet, csv]
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
        limits:
          cpu: 2000m
          memory: 4Gi
          
    production:
      config:
        storage:
          backend: s3
          bucket_name: stellar-production-ledgers
        processing:
          batch_size: 5000
          processor_threads: 16
          concurrent_readers: 8
        output:
          formats: [parquet]
          compression: zstd
      resources:
        requests:
          cpu: 2000m
          memory: 4Gi
        limits:
          cpu: 8000m
          memory: 16Gi
          
  # Quality assurance checks
  quality_checks:
    enabled: true
    
    # Data validation
    validation:
      - name: ledger_sequence_continuity
        description: "Ensure ledger sequences are continuous"
        enabled: true
        
      - name: event_count_validation
        description: "Validate event counts against expected ranges"
        enabled: true
        
      - name: schema_compliance
        description: "Ensure all records match Arrow schemas"
        enabled: true
        
    # Performance monitoring
    performance_checks:
      - name: throughput_monitoring
        description: "Monitor processing throughput"
        threshold: "1000 events/sec"
        
      - name: memory_usage
        description: "Monitor memory consumption"
        threshold: "80%"
        
      - name: error_rate
        description: "Monitor processing error rate"
        threshold: "1%"